---
title: Managing Large Files with Git & GitHub
---

::: {.unit-overview}
# Overview
Plain Git stores every version of every file. That makes **large or binary assets** (genomic FASTQs, high‑resolution images, video, machine‑learning weights) explode your repository’s size and slow down every clone.  
This unit shows reliable strategies—chiefly **Git LFS** and alternative storage—to keep history lean while preserving reproducibility.
:::

::: {.unit-goals}
# Goals
* Understand why vanilla Git struggles with large or binary files  
* Configure and use **Git Large File Storage (LFS)**  
* Compare Git LFS with other approaches (DVC, annex, object storage)  
* Apply best practices for dataset versioning and workflow reproducibility
:::

::: {.unit-reading}
# Reading

## 1  Why large files are a problem

* Git stores objects as delta‑compressed snapshots. Binary files do **not diff well**—every small edit triggers a full‑size blob.  
* Repository size affects _all_ contributors: clones, pulls, CI jobs.

| Size | Impact |
|------|--------|
| ≤ 100 MB | OK for GitHub |
| 100–500 MB | GitHub warns; LFS recommended |
| > 2 GB per repo / > 100 MB per file | Hard GitHub limit |

## 2  Git LFS essentials

1. **Install**  
   ```bash
   git lfs install
   ```
2. **Track patterns**  
   ```bash
   git lfs track "*.fastq.gz" "*.png"
   echo "*.fastq.gz filter=lfs diff=lfs merge=lfs -text" >> .gitattributes
   ```
3. **Add & commit** files as usual.  
   Git replaces the content with a small pointer file; binary data upload to the LFS server.

### Storage & bandwidth quotas

* Free GitHub accounts: 1 GB storage + 1 GB bandwidth/month.  
* Extra packs are cheap; or self‑host an LFS server.

## 3  Alternative solutions

| Tool | Best for | Notes |
|------|----------|-------|
| **DVC** | ML datasets & pipelines | Data tracked in remote blob storage (S3, GCS) |
| **git‑annex** | Offline archival | Uses symbolic links; steeper learning curve |
| **Rclone + checksum manifest** | Occasional big dumps | Simple, not integrated with Git history |

## 4  Tips & pitfalls

* **Add `.gitattributes` _before_ first commit**—converting history later is painful.  
* Keep raw data in *external* storage and version _metadata_ only.  
* Use **`.gitignore`** for scratch, cache, outputs; never commit derived artefacts.  
* CI: install `git‑lfs` and run `git lfs pull` before build steps.

## Summary

Git LFS (or similar tools) lets you keep code and data under one logical roof without bloating your repo. Adopt it early and automate retrieval in workflows.
:::

::: {.unit-resources}
# Further resources

* **Git LFS docs** – <https://git-lfs.github.com/>  
* **DVC** – <https://dvc.org/>  
* **GitHub Docs: Working with Large Files** – <https://docs.github.com/github/managing-large-files>  
* **BFG Repo‑Cleaner** – <https://rtyley.github.io/bfg-repo-cleaner/>
:::

::: {.unit-exercise}
# Practice

1. Clone <https://github.com/andreashandel/git-lfs-demo>.  
2. Add a large test file (> 150 MB) and track it with Git LFS.  
3. Push and verify the file appears as an LFS pointer in GitHub’s web UI.  
4. Configure a GitHub Action that pulls LFS objects before running an R script that uses the data.
:::
